{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import Sastrawi\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.tag import CRFTagger\n",
    "from collections import Counter\n",
    "import warnings\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentimen</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>oks kak semangat ya kalian kalian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>sekarang harus kaya orang bodoh lagi bodoh sangat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Begitu diumumkan lulus 100%, mereka semua suju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[USERNAME] [USERNAME] Katanya Bapak Reformasi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>macet macetan perut kosong akhirnya mampir dah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Pernyataan paling mengganggu telinga malam ini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Masi belum move on dari poto poto ini. Ceritan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Dibalik kecemburuan,terselip rasa kasih sayang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Kalo udah sayang beneran itu mau dihadapkan sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>[USERNAME] Pagi juga mas nyaa, duhh jadi gaena...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>Efek pintu tol slipi arah semanggi ditutup, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>jangan shyok ya :d rw yalah ka jadi selama ini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>nonton hanya tidak-dapat tiket nih hana :) hem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>Melakukan hal yang sebenernya lo ngga suka, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>bikin cake ulang tahun dan hasilnya bagus yeye :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>jadi pusing kana sirah na oge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>Jadi orang tua itu gitu ya. Mau anaknya sepert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>Romantis itu MANIS lebih manis lagi kalo dilak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>hari ini aku lelah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>Toh ya. Gue jelasin ke kalian kalau gue sayang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>Mama takut putri manisnya jatuh lalu terluka. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>oalah pantas iyanih dek kan lagi promosi sambi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>Kalo ngga terpaksa juga ngga bakal gue naik bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>Sekelas emerson masih gitu kelakuannya, apa la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>sudah ah jangan sedih katanya strong sel -adit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>cowok ganteng adalah yang pergi solat jum'at s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>lumayan loh ini cuci sendirian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>Dari semua pakaian yang ada di pasar Bayangkar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>Gara gara kapan hari ngopi trus dengerin lagu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>Katherine sofia setelah bener\" ngrasa capek bg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3432</th>\n",
       "      <td>3433</td>\n",
       "      <td>0</td>\n",
       "      <td>cie sombong gamaujawab ciye jadisombongyasekar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3433</th>\n",
       "      <td>3434</td>\n",
       "      <td>1</td>\n",
       "      <td>Kl mudik blk ke sby logistik bahan pangan leng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3434</th>\n",
       "      <td>3435</td>\n",
       "      <td>0</td>\n",
       "      <td>[Askmf] coba deh sekali aja lu jd gue, ngrasai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3435</th>\n",
       "      <td>3436</td>\n",
       "      <td>0</td>\n",
       "      <td>... serioulsy. Ketika ada yang speak up kalau ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3436</th>\n",
       "      <td>3437</td>\n",
       "      <td>0</td>\n",
       "      <td>Kebiasaan. Macet bgt sih. Katanya ibu kota3-| ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3437</th>\n",
       "      <td>3438</td>\n",
       "      <td>0</td>\n",
       "      <td>Wanita itu pelit sekali tdk mau mmberi tau ara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3438</th>\n",
       "      <td>3439</td>\n",
       "      <td>1</td>\n",
       "      <td>Jaramg bnget cafe puter lagu glenn kaya dsini,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3439</th>\n",
       "      <td>3440</td>\n",
       "      <td>0</td>\n",
       "      <td>Kalo inget hari itu worst bgt sih. Kebetulan e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3440</th>\n",
       "      <td>3441</td>\n",
       "      <td>1</td>\n",
       "      <td>Kalau kamu sayang seseorang,mqu gak kamu nungg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3441</th>\n",
       "      <td>3442</td>\n",
       "      <td>0</td>\n",
       "      <td>dari kemarin mao ganti ava susah sekali sudah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3442</th>\n",
       "      <td>3443</td>\n",
       "      <td>0</td>\n",
       "      <td>Entah kenapa kalau jam segini belum bisa tidur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3443</th>\n",
       "      <td>3444</td>\n",
       "      <td>0</td>\n",
       "      <td>[USERNAME] Tapi suka ku :( inya tu dingin orgn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3444</th>\n",
       "      <td>3445</td>\n",
       "      <td>1</td>\n",
       "      <td>Waktu itu rooben goalin gawang kurnia meiga de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3445</th>\n",
       "      <td>3446</td>\n",
       "      <td>1</td>\n",
       "      <td>Harusnya kita appreciate akhirnya ada yang bic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3446</th>\n",
       "      <td>3447</td>\n",
       "      <td>0</td>\n",
       "      <td>Kita masih mampu beli ini itu liburan dg uang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3447</th>\n",
       "      <td>3448</td>\n",
       "      <td>1</td>\n",
       "      <td>Setiap perempuan cantik dengan caranya sendiri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3448</th>\n",
       "      <td>3449</td>\n",
       "      <td>1</td>\n",
       "      <td>Senang bisa menimba ilmu dari guru besar, semo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3449</th>\n",
       "      <td>3450</td>\n",
       "      <td>0</td>\n",
       "      <td>halah cewe jaman sekarang di-catcall dikit lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3450</th>\n",
       "      <td>3451</td>\n",
       "      <td>1</td>\n",
       "      <td>Oh Tuhan, Ku cinta dia, Ku sayang dia, Rindu d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3451</th>\n",
       "      <td>3452</td>\n",
       "      <td>1</td>\n",
       "      <td>Oh seperti itu a. Seru dong ya Semoga terus su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3452</th>\n",
       "      <td>3453</td>\n",
       "      <td>0</td>\n",
       "      <td>pen deh membicarakan semua begitu biar jelas m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3453</th>\n",
       "      <td>3454</td>\n",
       "      <td>1</td>\n",
       "      <td>Musim terindah adalah Ketika kau nyalakan pagi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3454</th>\n",
       "      <td>3455</td>\n",
       "      <td>1</td>\n",
       "      <td>Sama aja ketika kita sayang samaa org, kita ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3455</th>\n",
       "      <td>3456</td>\n",
       "      <td>1</td>\n",
       "      <td>martabak telur enak kali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3456</th>\n",
       "      <td>3457</td>\n",
       "      <td>0</td>\n",
       "      <td>RI siap menaikkan harga bbm.Emang rakyat mau a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3457</th>\n",
       "      <td>3458</td>\n",
       "      <td>0</td>\n",
       "      <td>[USERNAME] tolong aplikasinya diberesin. Saya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3458</th>\n",
       "      <td>3459</td>\n",
       "      <td>0</td>\n",
       "      <td>Ntan busuk banget, temen kok kamu gituin. Aku ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3459</th>\n",
       "      <td>3460</td>\n",
       "      <td>1</td>\n",
       "      <td>Makin salut aje ane sama jokowi...  Seribu sat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3460</th>\n",
       "      <td>3461</td>\n",
       "      <td>0</td>\n",
       "      <td>wkowko sudah salah pergaulan dia kali ya sakit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3461</th>\n",
       "      <td>3462</td>\n",
       "      <td>1</td>\n",
       "      <td>Sebagai dokter gue punya kelegaan tersendiri k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3462 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  sentimen                                              tweet\n",
       "0        1         1                  oks kak semangat ya kalian kalian\n",
       "1        2         0  sekarang harus kaya orang bodoh lagi bodoh sangat\n",
       "2        3         1  Begitu diumumkan lulus 100%, mereka semua suju...\n",
       "3        4         0  [USERNAME] [USERNAME] Katanya Bapak Reformasi ...\n",
       "4        5         0  macet macetan perut kosong akhirnya mampir dah...\n",
       "5        6         0  Pernyataan paling mengganggu telinga malam ini...\n",
       "6        7         1  Masi belum move on dari poto poto ini. Ceritan...\n",
       "7        8         1  Dibalik kecemburuan,terselip rasa kasih sayang...\n",
       "8        9         1  Kalo udah sayang beneran itu mau dihadapkan sa...\n",
       "9       10         1  [USERNAME] Pagi juga mas nyaa, duhh jadi gaena...\n",
       "10      11         0  Efek pintu tol slipi arah semanggi ditutup, ma...\n",
       "11      12         0  jangan shyok ya :d rw yalah ka jadi selama ini...\n",
       "12      13         1  nonton hanya tidak-dapat tiket nih hana :) hem...\n",
       "13      14         1  Melakukan hal yang sebenernya lo ngga suka, de...\n",
       "14      15         1  bikin cake ulang tahun dan hasilnya bagus yeye :)\n",
       "15      16         0                      jadi pusing kana sirah na oge\n",
       "16      17         1  Jadi orang tua itu gitu ya. Mau anaknya sepert...\n",
       "17      18         1  Romantis itu MANIS lebih manis lagi kalo dilak...\n",
       "18      19         0                                 hari ini aku lelah\n",
       "19      20         0  Toh ya. Gue jelasin ke kalian kalau gue sayang...\n",
       "20      21         1  Mama takut putri manisnya jatuh lalu terluka. ...\n",
       "21      22         1  oalah pantas iyanih dek kan lagi promosi sambi...\n",
       "22      23         0  Kalo ngga terpaksa juga ngga bakal gue naik bu...\n",
       "23      24         0  Sekelas emerson masih gitu kelakuannya, apa la...\n",
       "24      25         0     sudah ah jangan sedih katanya strong sel -adit\n",
       "25      26         1  cowok ganteng adalah yang pergi solat jum'at s...\n",
       "26      27         0                    lumayan loh ini cuci sendirian \n",
       "27      28         1  Dari semua pakaian yang ada di pasar Bayangkar...\n",
       "28      29         1  Gara gara kapan hari ngopi trus dengerin lagu ...\n",
       "29      30         1  Katherine sofia setelah bener\" ngrasa capek bg...\n",
       "...    ...       ...                                                ...\n",
       "3432  3433         0  cie sombong gamaujawab ciye jadisombongyasekar...\n",
       "3433  3434         1  Kl mudik blk ke sby logistik bahan pangan leng...\n",
       "3434  3435         0  [Askmf] coba deh sekali aja lu jd gue, ngrasai...\n",
       "3435  3436         0  ... serioulsy. Ketika ada yang speak up kalau ...\n",
       "3436  3437         0  Kebiasaan. Macet bgt sih. Katanya ibu kota3-| ...\n",
       "3437  3438         0  Wanita itu pelit sekali tdk mau mmberi tau ara...\n",
       "3438  3439         1  Jaramg bnget cafe puter lagu glenn kaya dsini,...\n",
       "3439  3440         0  Kalo inget hari itu worst bgt sih. Kebetulan e...\n",
       "3440  3441         1  Kalau kamu sayang seseorang,mqu gak kamu nungg...\n",
       "3441  3442         0      dari kemarin mao ganti ava susah sekali sudah\n",
       "3442  3443         0  Entah kenapa kalau jam segini belum bisa tidur...\n",
       "3443  3444         0  [USERNAME] Tapi suka ku :( inya tu dingin orgn...\n",
       "3444  3445         1  Waktu itu rooben goalin gawang kurnia meiga de...\n",
       "3445  3446         1  Harusnya kita appreciate akhirnya ada yang bic...\n",
       "3446  3447         0  Kita masih mampu beli ini itu liburan dg uang ...\n",
       "3447  3448         1  Setiap perempuan cantik dengan caranya sendiri...\n",
       "3448  3449         1  Senang bisa menimba ilmu dari guru besar, semo...\n",
       "3449  3450         0  halah cewe jaman sekarang di-catcall dikit lan...\n",
       "3450  3451         1  Oh Tuhan, Ku cinta dia, Ku sayang dia, Rindu d...\n",
       "3451  3452         1  Oh seperti itu a. Seru dong ya Semoga terus su...\n",
       "3452  3453         0  pen deh membicarakan semua begitu biar jelas m...\n",
       "3453  3454         1  Musim terindah adalah Ketika kau nyalakan pagi...\n",
       "3454  3455         1  Sama aja ketika kita sayang samaa org, kita ba...\n",
       "3455  3456         1                           martabak telur enak kali\n",
       "3456  3457         0  RI siap menaikkan harga bbm.Emang rakyat mau a...\n",
       "3457  3458         0  [USERNAME] tolong aplikasinya diberesin. Saya ...\n",
       "3458  3459         0  Ntan busuk banget, temen kok kamu gituin. Aku ...\n",
       "3459  3460         1  Makin salut aje ane sama jokowi...  Seribu sat...\n",
       "3460  3461         0  wkowko sudah salah pergaulan dia kali ya sakit...\n",
       "3461  3462         1  Sebagai dokter gue punya kelegaan tersendiri k...\n",
       "\n",
       "[3462 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = pd.read_csv('train_set.csv', encoding=\"Latin-1\")\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweet(tweet):\n",
    "    token = nltk.word_tokenize(tweet)\n",
    "    tokenized_tweet = ' '.join(token)\n",
    "    return tokenized_tweet\n",
    "\n",
    "def normalisasi(tweet): #normalisasi 1 tweet\n",
    "    normal_tw = tweet.lower() #lowercase\n",
    "    normal_tw = re.sub('[^\\x30-\\x39^\\x41-\\x5A^\\x61-\\x7A^\\s^-]','',normal_tw) #buang punctuation dan karakter selain A-Z, a-z, dan 0-9\n",
    "    normal_tw = re.sub('tidak-','tidak ',normal_tw)\n",
    "    normal_tw = re.sub('\\d','',normal_tw)\n",
    "    normal_tw = re.sub('\\s+', ' ', normal_tw) # remove extra space\n",
    "    normal_tw = normal_tw.strip() #trim depan belakang\n",
    "    normal_regex = re.compile(r\"(.)\\1{1,}\", re.IGNORECASE) #regex huruf yang berulang kaya haiiii (untuk fitur unigram)\n",
    "    normal_tw = normal_regex.sub(r\"\\1\\1\", normal_tw) #buang huruf yang berulang\n",
    "    return normal_tw\n",
    "\n",
    "def do_normalization(raw_tweet): #normalisasi banyak tweet\n",
    "    normalized_tweet = []\n",
    "    for tw in raw_tweet:\n",
    "        tokenized_tweet = tokenize_tweet(tw)\n",
    "        normal_tweet = normalisasi(tokenized_tweet)\n",
    "        normalized_tweet.append(normal_tweet)\n",
    "    print(\"Normalization Done\")\n",
    "    return normalized_tweet\n",
    "\n",
    "def stemming(tweet): #stemming 1 tweet\n",
    "    token = nltk.word_tokenize(tweet)\n",
    "    stem_kalimat = []\n",
    "    for k in token:\n",
    "        factory = StemmerFactory()\n",
    "        stemmer = factory.create_stemmer()\n",
    "        stem_kata = stemmer.stem(k)\n",
    "        stem_kalimat.append(stem_kata)\n",
    "    stem_kalimat_str = ' '.join(stem_kalimat)\n",
    "    return stem_kalimat_str\n",
    "\n",
    "def do_stemming(list_tweet): #stemming banyak tweet\n",
    "    tweet_result = []\n",
    "    counter_stem = 1\n",
    "    for tw in list_tweet:\n",
    "        print(counter_stem)\n",
    "        print(\"Sentences to be stemmed : \",tw)\n",
    "        stemming_result = stemming(tw)\n",
    "        tweet_result.append(stemming_result)\n",
    "        counter_stem += 1\n",
    "    return tweet_result\n",
    "\n",
    "def remove_stopwords(tweet): #remove stopwords 1 tweet\n",
    "    stopwords = pd.read_csv(\"filtered_stopwords.csv\", names=['stopword'])['stopword'].tolist()\n",
    "    special_list = ['username', 'url', 'sensitive-no','askmf']\n",
    "    token = nltk.word_tokenize(tweet)\n",
    "    token_afterremoval = []\n",
    "    for k in token:\n",
    "        if k not in stopwords and k not in special_list:\n",
    "            token_afterremoval.append(k)\n",
    "    str_clean = ' '.join(token_afterremoval)\n",
    "    return str_clean\n",
    "def do_remove_stopwords(list_tweet): #remove stopwords banyak tweet\n",
    "    tweet_result = []\n",
    "    for tw in list_tweet:\n",
    "        remove_result = remove_stopwords(tw)\n",
    "        tweet_result.append(remove_result)\n",
    "    print(\"Stopword Removal Done\")\n",
    "    return tweet_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_stemming(list_tweet):\n",
    "    normalized_tweet = do_normalization(list_tweet)\n",
    "    stopword_removed_tweet = do_remove_stopwords(normalized_tweet)\n",
    "    stemming_processed_tweet = do_stemming(stopword_removed_tweet)\n",
    "    return stemming_processed_tweet\n",
    "def preprocess_without_stemming(list_tweet):\n",
    "    normalized_tweet = do_normalization(list_tweet)\n",
    "    stopword_removed_tweet = do_remove_stopwords(normalized_tweet)\n",
    "    return stopword_removed_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tweet = tweet['tweet']\n",
    "# stemming_tweet= preprocess_with_stemming(raw_tweet)\n",
    "no_stemming_tweet = preprocess_without_stemming(raw_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_count(no_stemming_tweet): #distribusi vocab beserta count nya di dalam corpus train\n",
    "    vocab = {}\n",
    "    for twit in no_stemming_tweet:\n",
    "        for kata in twit.split(\" \"):\n",
    "            if kata not in vocab:\n",
    "                vocab[kata] = 1\n",
    "            else:\n",
    "                vocab[kata] += 1\n",
    "    return vocab\n",
    "\n",
    "def create_vocab_df(vocab):\n",
    "    tmp_vocab = []\n",
    "    for kata in vocab:\n",
    "        tmp_vocab.append([kata,vocab[kata]])\n",
    "    vocab_df = pd.DataFrame(tmp_vocab, columns=['vocab','count']).sort_values('count',ascending=False)\n",
    "    return vocab_df\n",
    "\n",
    "vocab = create_vocab_count(no_stemming_tweet)\n",
    "vocab_df = create_vocab_df(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#secara keseluruhan, kelima method ini melakukan filter sehingga tersisa vocab adjective beserta count nya saja\n",
    "def EkstraksiPOS(vocab,list_vocab):\n",
    "    ct = CRFTagger()\n",
    "    ct.set_model_file(\"all_indo_man_tag_corpus_model.crf.tagger\")\n",
    "    pos_tag = ct.tag_sents([list_vocab])[0]\n",
    "    return pos_tag\n",
    "def tuple_to_list_pos(list_pos_tag):\n",
    "    pos_tag = []\n",
    "    for tup in list_pos_tag:\n",
    "        pos_tag.append(list(tup))\n",
    "    return pos_tag\n",
    "def add_count_to_pos(list_pos_tag, vocab):\n",
    "    for idx in range(len(list_pos_tag)):\n",
    "        list_pos_tag[idx].append(vocab[list_pos_tag[idx][0]])\n",
    "    return list_pos_tag\n",
    "def just_take_jj_tag(list_pos_tag):\n",
    "    pos_tag = []\n",
    "    for idx in range(len(list_pos_tag)):\n",
    "        if(list_pos_tag[idx][1] == 'JJ'):\n",
    "            pos_tag.append(list_pos_tag[idx])\n",
    "    print(pos_tag)\n",
    "    return pos_tag\n",
    "def save_filtered_pos_tag(list_pos_tag):\n",
    "    filtered_pos_tag = pd.DataFrame(list_pos_tag, columns=['vocab','pos','count'])\n",
    "    filtered_pos_tag.to_csv('vocab_adj.csv', index=False)\n",
    "\n",
    "pos_tag = EkstraksiPOS(vocab, vocab_df['vocab'].tolist())\n",
    "pos_tag = tuple_to_list_pos(pos_tag)\n",
    "pos_tag = add_count_to_pos(pos_tag,vocab)\n",
    "pos_tag = just_take_jj_tag(pos_tag)\n",
    "save_filtered_pos_tag(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_norm_mapping(): #buat dictionary untuk proses normalisasi lebih lanjut\n",
    "    file = open(\"normalisasi_mapping.txt\", \"r\")\n",
    "    line = file.readline()\n",
    "    mapping = []\n",
    "    while line:\n",
    "        tmp = line.split(\",\")\n",
    "        tmp[-1] = tmp[-1][:-1]\n",
    "        mapping.append(tmp)\n",
    "        line = file.readline()\n",
    "    file.close()\n",
    "    print(mapping)\n",
    "    norm_mapping = {}\n",
    "    for lst in mapping:\n",
    "        for kata in lst[:-1]:\n",
    "            norm_mapping[kata] = lst[-1]\n",
    "    print(norm_mapping)\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_normalisasi(tweet, norm_mapping):\n",
    "    cleaned_tweet= []\n",
    "    for tw in tweet:\n",
    "        new_tweet = []\n",
    "        for kata in tw.split(\" \"):\n",
    "            if kata not in norm_mapping:\n",
    "                new_tweet.append(kata)\n",
    "            else:\n",
    "                new_tweet.append(norm_mapping[kata])\n",
    "        cleaned_tweet.append(\" \".join(new_tweet))\n",
    "    return cleaned_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['yang', 'yg', 'yang'], ['dan', 'and', 'n', 'dan'], ['sy', 'saya', 'aq', 'aku', 'gw', 'gua', 'gue', 'saya'], ['itu', 'tu', 'gitu', 'gt', 'itu'], ['kamu', 'qm', 'km', 'mu', 'lu', 'lo', 'elu', 'elo', 'kamu'], ['sama', 'sm', 'sma', 'sama'], ['tidak', 'ga', 'gk', 'gak', 'tdk', 'tidak'], ['bs', 'bisa', 'sabi', 'bisa'], ['mau', 'mw', 'ingin', 'pengen', 'pgn', 'pingin', 'mau'], ['lg', 'lagi', 'again', 'lagi'], ['tapi', 'tp', 'but', 'tapi'], ['dia', 'dy', 'dia'], ['kita', 'kite', 'qt', 'kita'], ['syg', 'sayang', 'sayank', 'sayang'], ['orang', 'org', 'orang'], ['saja', 'aja', 'aj', 'sj', 'saja'], ['jg', 'juga', 'also', 'juga'], ['dari', 'dr', 'from', 'dari'], ['udh', 'sdh', 'sudah', 'udah', 'udeh', 'sudeh', 'sudah'], ['jadi', 'jd', 'jadi'], ['kalau', 'kalo', 'kl', 'if', 'kalau'], ['dengan', 'dgn', 'with', 'dengan'], ['apa', 'ape', 'what', 'apa'], ['buat', 'bwt', 'buat'], ['selamat', 'slmt', 'met', 'slamat', 'selamat'], ['msh', 'masih', 'masih'], ['banget', 'bgt', 'very', 'banget'], ['selalu', 'slalu', 'clalu', 'selalu'], ['jangan', 'jgn', 'dont', 'jangan'], ['karena', 'because', 'coz', 'krn', 'karena'], ['banyak', 'byk', 'much', 'bjibun', 'bejibun', 'banyak'], ['terus', 'trs', 'mulu', 'molo', 'mlu', 'terus'], ['baik', 'baek', 'bae', 'baik'], ['aduh', 'duh', 'haduh', 'waduh'], ['gmn', 'bgmn', 'bagaimana'], ['mangat', 'smngt', 'semangat'], ['bgs', 'bgus', 'bagus'], ['kali', 'kli', 'bgt', 'banget', 'sekali'], ['slah', 'salah'], ['bru', 'br', 'baru'], ['bnr', 'bner', 'bnar', 'btul', 'benar'], ['pnting', 'penting'], ['jls', 'jlas', 'jelas'], ['pnjang', 'pnjg', 'panjang'], ['pnuh', 'penuh'], ['kcil', 'kecil'], ['cpt', 'cpet', 'cepet', 'cepat'], ['srius', 'sriuz', 'serius'], ['knal', 'kenal'], ['ggal', 'gagal'], ['pusink', 'pusing'], ['mntn', 'mntan', 'mantan'], ['lncr', 'lancar'], ['ykin', 'yakin'], ['jjur', 'jujur'], ['tpt', 'tepat'], ['brat', 'berat'], ['bruk', 'jelek', 'jlek', 'buruk'], ['mdah', 'mudah'], ['tlus', 'tulus'], ['skit', 'sakit'], ['maen', 'main'], ['shat', 'sehat'], ['smbil', 'sambil'], ['pcr', 'pcar', 'pacar'], ['suksez', 'skses', 'sukses'], ['tbaik', 'trbaik', 'terbaek', 'terbaik'], ['gede', 'gde', 'besar'], ['strong', 'kuat'], ['kls', 'kelas'], ['nngis', 'nangis'], ['trakhir', 'trkhir', 'last', 'terakhir'], ['kmrn', 'kemari']]\n",
      "{'yang': 'yang', 'yg': 'yang', 'dan': 'dan', 'and': 'dan', 'n': 'dan', 'sy': 'saya', 'saya': 'saya', 'aq': 'saya', 'aku': 'saya', 'gw': 'saya', 'gua': 'saya', 'gue': 'saya', 'itu': 'itu', 'tu': 'itu', 'gitu': 'itu', 'gt': 'itu', 'kamu': 'kamu', 'qm': 'kamu', 'km': 'kamu', 'mu': 'kamu', 'lu': 'kamu', 'lo': 'kamu', 'elu': 'kamu', 'elo': 'kamu', 'sama': 'sama', 'sm': 'sama', 'sma': 'sama', 'tidak': 'tidak', 'ga': 'tidak', 'gk': 'tidak', 'gak': 'tidak', 'tdk': 'tidak', 'bs': 'bisa', 'bisa': 'bisa', 'sabi': 'bisa', 'mau': 'mau', 'mw': 'mau', 'ingin': 'mau', 'pengen': 'mau', 'pgn': 'mau', 'pingin': 'mau', 'lg': 'lagi', 'lagi': 'lagi', 'again': 'lagi', 'tapi': 'tapi', 'tp': 'tapi', 'but': 'tapi', 'dia': 'dia', 'dy': 'dia', 'kita': 'kita', 'kite': 'kita', 'qt': 'kita', 'syg': 'sayang', 'sayang': 'sayang', 'sayank': 'sayang', 'orang': 'orang', 'org': 'orang', 'saja': 'saja', 'aja': 'saja', 'aj': 'saja', 'sj': 'saja', 'jg': 'juga', 'juga': 'juga', 'also': 'juga', 'dari': 'dari', 'dr': 'dari', 'from': 'dari', 'udh': 'sudah', 'sdh': 'sudah', 'sudah': 'sudah', 'udah': 'sudah', 'udeh': 'sudah', 'sudeh': 'sudah', 'jadi': 'jadi', 'jd': 'jadi', 'kalau': 'kalau', 'kalo': 'kalau', 'kl': 'kalau', 'if': 'kalau', 'dengan': 'dengan', 'dgn': 'dengan', 'with': 'dengan', 'apa': 'apa', 'ape': 'apa', 'what': 'apa', 'buat': 'buat', 'bwt': 'buat', 'selamat': 'selamat', 'slmt': 'selamat', 'met': 'selamat', 'slamat': 'selamat', 'msh': 'masih', 'masih': 'masih', 'banget': 'sekali', 'bgt': 'sekali', 'very': 'banget', 'selalu': 'selalu', 'slalu': 'selalu', 'clalu': 'selalu', 'jangan': 'jangan', 'jgn': 'jangan', 'dont': 'jangan', 'karena': 'karena', 'because': 'karena', 'coz': 'karena', 'krn': 'karena', 'banyak': 'banyak', 'byk': 'banyak', 'much': 'banyak', 'bjibun': 'banyak', 'bejibun': 'banyak', 'terus': 'terus', 'trs': 'terus', 'mulu': 'terus', 'molo': 'terus', 'mlu': 'terus', 'baik': 'baik', 'baek': 'baik', 'bae': 'baik', 'aduh': 'waduh', 'duh': 'waduh', 'haduh': 'waduh', 'gmn': 'bagaimana', 'bgmn': 'bagaimana', 'mangat': 'semangat', 'smngt': 'semangat', 'bgs': 'bagus', 'bgus': 'bagus', 'kali': 'sekali', 'kli': 'sekali', 'slah': 'salah', 'bru': 'baru', 'br': 'baru', 'bnr': 'benar', 'bner': 'benar', 'bnar': 'benar', 'btul': 'benar', 'pnting': 'penting', 'jls': 'jelas', 'jlas': 'jelas', 'pnjang': 'panjang', 'pnjg': 'panjang', 'pnuh': 'penuh', 'kcil': 'kecil', 'cpt': 'cepat', 'cpet': 'cepat', 'cepet': 'cepat', 'srius': 'serius', 'sriuz': 'serius', 'knal': 'kenal', 'ggal': 'gagal', 'pusink': 'pusing', 'mntn': 'mantan', 'mntan': 'mantan', 'lncr': 'lancar', 'ykin': 'yakin', 'jjur': 'jujur', 'tpt': 'tepat', 'brat': 'berat', 'bruk': 'buruk', 'jelek': 'buruk', 'jlek': 'buruk', 'mdah': 'mudah', 'tlus': 'tulus', 'skit': 'sakit', 'maen': 'main', 'shat': 'sehat', 'smbil': 'sambil', 'pcr': 'pacar', 'pcar': 'pacar', 'suksez': 'sukses', 'skses': 'sukses', 'tbaik': 'terbaik', 'trbaik': 'terbaik', 'terbaek': 'terbaik', 'gede': 'besar', 'gde': 'besar', 'strong': 'kuat', 'kls': 'kelas', 'nngis': 'nangis', 'trakhir': 'terakhir', 'trkhir': 'terakhir', 'last': 'terakhir', 'kmrn': 'kemari'}\n"
     ]
    }
   ],
   "source": [
    "norm_map = create_norm_mapping()\n",
    "# cleaned_stemming = adv_normalisasi(stemming_tweet,norm_map)\n",
    "cleaned_stemming = pd.read_csv('cleaned_stemming.csv', names=['id','tweet'])['tweet'].tolist()\n",
    "# cleaned_no_stemming = adv_normalisasi(no_stemming_tweet,norm_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#membuat meaning dictionary sehingga vocab adjective thesaurus.json yang saling bersinonim di-mapping ke meaning yang sama\n",
    "def generate_meaning_dict():\n",
    "    with open('thesaurus.json', 'r') as f:\n",
    "        thesaurus = json.load(f)\n",
    "    adj_thesaurus = {}\n",
    "    for kata in thesaurus:\n",
    "        if(thesaurus[kata]['tag'] == 'a'):\n",
    "            adj_thesaurus[kata] = thesaurus[kata]\n",
    "    meaning_dict = {}\n",
    "    counter_meaning = 0\n",
    "    meaning = \"m\"\n",
    "    for kata in adj_thesaurus:\n",
    "        if(kata not in meaning_dict):\n",
    "            counter_meaning += 1\n",
    "            meaning = \"m\" + str(counter_meaning)\n",
    "            meaning_dict[kata] = meaning\n",
    "        for sinonim in adj_thesaurus[kata]['sinonim']:\n",
    "            if(sinonim not in meaning_dict):\n",
    "                meaning_dict[sinonim] = meaning_dict[kata]\n",
    "    return meaning_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalisasi_with_sinonim(cleaned_no_stemming, meaning_dict): #normalisasi tweet berdasarkan meaning dictionary\n",
    "    tweet_sinonim_normalized = []\n",
    "    for tweet in cleaned_no_stemming:\n",
    "        new_tweet = []\n",
    "        for kata in tweet.split(\" \"):\n",
    "            if kata not in meaning_dict:\n",
    "                new_tweet.append(kata)\n",
    "            else:\n",
    "                new_tweet.append(meaning_dict[kata])\n",
    "        tweet_sinonim_normalized.append(\" \".join(new_tweet))\n",
    "    return tweet_sinonim_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaning_dict = generate_meaning_dict()\n",
    "sinonim_normalized = normalisasi_with_sinonim(cleaned_no_stemming,meaning_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet['cleaned_stemming'] = cleaned_stemming\n",
    "tweet['cleaned_no_stemming'] = cleaned_no_stemming\n",
    "tweet['sinonim_normalized'] = sinonim_normalized\n",
    "\n",
    "tweet['cleaned_stemming'].to_csv('cleaned_stemming.csv', index=False)\n",
    "tweet['cleaned_no_stemming'].to_csv('cleaned_no_stemming.csv', index=False)\n",
    "tweet['sinonim_normalized'].to_csv('sinonim_normalized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_test = pd.read_csv('test_set.csv', encoding=\"Latin-1\")\n",
    "raw_test = tweet_test['tweet']\n",
    "\n",
    "no_stemming_tweet_test = preprocess_without_stemming(raw_test)\n",
    "# stemming_tweet_test = preprocess_with_stemming(raw_test)\n",
    "\n",
    "# cleaned_stemming_test = adv_normalisasi(stemming_tweet_test,norm_map)\n",
    "cleaned_stemming_test = pd.read_csv('cleaned_stemming_test.csv')['cleaned_tweet_test'].tolist()\n",
    "cleaned_no_stemming_test = adv_normalisasi(no_stemming_tweet_test,norm_map)\n",
    "sinonim_normalized_test = normalisasi_with_sinonim(cleaned_no_stemming_test,meaning_dict)\n",
    "\n",
    "tweet_stemming = cleaned_stemming + cleaned_stemming_test\n",
    "tweet_no_stemming = cleaned_no_stemming + cleaned_no_stemming_test\n",
    "tweet_sinonim_normalized = sinonim_normalized + sinonim_normalized_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EkstraksiBoW(tweet):\n",
    "    unigram = CountVectorizer(ngram_range=(1,1), max_features=5000)\n",
    "    unigram_matrix = unigram.fit_transform(np.array(tweet)).todense()\n",
    "    nama_fitur = unigram.get_feature_names()\n",
    "    return unigram_matrix, nama_fitur\n",
    "def create_result_nb(y_pred):\n",
    "    id_tweet = 0\n",
    "    sentiment_label = []\n",
    "    for pred in y_pred:\n",
    "        sentiment_label.append([id_tweet,pred])\n",
    "        id_tweet += 1\n",
    "    result_df = pd.DataFrame(sentiment_label)\n",
    "    result_df.to_csv('test_result.csv', index=False)\n",
    "\n",
    "unigram_stemming, feat_name_stemming = EkstraksiBoW(tweet_stemming)\n",
    "unigram_no_stemming, feat_name_no_stemming = EkstraksiBoW(tweet_no_stemming)\n",
    "unigram_sinonim, feat_name_sinonim = EkstraksiBoW(tweet_sinonim_normalized)\n",
    "\n",
    "unigram_stemming_train = unigram_stemming[:-8000]\n",
    "unigram_stemming_test = unigram_stemming[-8000:]\n",
    "\n",
    "unigram_no_stemming_train = unigram_no_stemming[:-8000]\n",
    "unigram_no_stemming_test = unigram_no_stemming[-8000:]\n",
    "\n",
    "unigram_sinonim_train = unigram_sinonim[:-8000]\n",
    "unigram_sinonim_test = unigram_sinonim[-8000:]\n",
    "\n",
    "clf_stemming = MultinomialNB()\n",
    "clf_stemming.fit(unigram_stemming_train, tweet['sentimen'])\n",
    "y_pred_stemming = clf_stemming.predict(unigram_stemming_test)\n",
    "\n",
    "clf_no_stemming = MultinomialNB()\n",
    "clf_no_stemming.fit(unigram_no_stemming_train, tweet['sentimen'])\n",
    "y_pred_no_stemming = clf_no_stemming.predict(unigram_no_stemming_test)\n",
    "\n",
    "clf_sinonim = MultinomialNB()\n",
    "clf_sinonim.fit(unigram_sinonim_train, tweet['sentimen'])\n",
    "y_pred_sinonim = clf_sinonim.predict(unigram_sinonim_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_result_nb(y_pred_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_result_nb(y_pred_no_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_result_nb(y_pred_sinonim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSentimenDict(): #membuat kamus sentimen\n",
    "    pos = pd.read_csv(\"positive.csv\", names=['pos'])\n",
    "    neg = pd.read_csv(\"negative.csv\", names=['neg'])\n",
    "    \n",
    "    list_pos = pos['pos'].tolist()\n",
    "    list_neg = neg['neg'].tolist()\n",
    "\n",
    "    sentimen_dict = dict()\n",
    "    sentimen_dict['pos'] = list_pos\n",
    "    sentimen_dict['neg'] = list_neg\n",
    "    \n",
    "    return sentimen_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EkstraksiSentimenNew(list_tweet,sentimen_dict): #ekstrasi fitur sentimen berdasarkan kamus sentimen\n",
    "    fitur_sentimen_all = []\n",
    "    for tweet in list_tweet:\n",
    "    ##inisiasi value\n",
    "        emosi = [\"positif\", \"negatif\"]\n",
    "        value = [0,0]\n",
    "        emosi_value = {}\n",
    "        for i in range(len(emosi)):\n",
    "            emosi_value[emosi[i]] = value[i]\n",
    "        list_kata = tweet.split()\n",
    "        for k in list_kata:\n",
    "            if k in sentimen_dict['pos']:\n",
    "                emosi_value[\"positif\"] += 1\n",
    "            if k in sentimen_dict['neg']:\n",
    "                emosi_value[\"negatif\"] += 1\n",
    "        fitur_sentimen_perkalimat = list(emosi_value.values())\n",
    "        fitur_sentimen_all.append(fitur_sentimen_perkalimat)\n",
    "    return fitur_sentimen_all\n",
    "def create_result(fitur_sentimen_all): #membuat hasil prediksi berdasarkan output fitur sentimen\n",
    "    sentiment_label = []\n",
    "    id_tweet = 0\n",
    "    counter_same = 0\n",
    "    for skor in fitur_sentimen_all:\n",
    "        if(skor[0] > skor[1]):\n",
    "            sentiment_label.append([id_tweet,1])\n",
    "        else:\n",
    "            sentiment_label.append([id_tweet,0])\n",
    "        id_tweet += 1\n",
    "        if(skor[0] == skor[1]):\n",
    "            counter_same += 1\n",
    "    result_df = pd.DataFrame(sentiment_label)\n",
    "    result_df.to_csv('fitur_sentimen_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimen_dict = generateSentimenDict()\n",
    "fitur_sentimen = EkstraksiSentimenNew(no_stemming_tweet_test, sentimen_dict)\n",
    "create_result(fitur_sentimen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
